{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating a customer base into different segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze a wholesale customers dataset with their annual spendings in various product categories. The task is to find the underlying structure in the data to optimize a new delivery method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "np.random.seed(42)\n",
    "import seaborn as sns\n",
    "\n",
    "# Import supplementary visualizations code visuals.py\n",
    "import visuals as vs\n",
    "import preprocessing as prep\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import a dataset with the anual spendings of various customers\n",
    "try:\n",
    "    data = pd.read_csv(\"customers.csv\")\n",
    "    data.drop(['Region', 'Channel'], axis = 1, inplace = True)\n",
    "    print(\"Wholesale customers dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "except:\n",
    "    print(\"Dataset could not be loaded. Is the dataset missing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial inspection of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select 3 different samples from the dataset and explore their differences\n",
    "indices = [2, 12, 72]\n",
    "\n",
    "# Create a DataFrame of the chosen samples\n",
    "samples = pd.DataFrame(data.loc[indices], columns = data.keys()).reset_index(drop = True)\n",
    "display(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore the distributions of the different features and try to find correlations between the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a scatter matrix for visualizing the correlations between the features\n",
    "plt.style.use('default')\n",
    "pd.plotting.scatter_matrix(data, alpha = 0.3, figsize = (8, 8), diagonal = 'kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Try to analyze the distributions. Are the distributions skewed or are there outliers in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distributions are skewed and there are outliers, so we apply some\n",
    "# preprocessing (log transformation, outlier removal)\n",
    "data, samples, outliers = prep.preprocess_data(data, samples)\n",
    "\n",
    "# And display the data again\n",
    "pd.plotting.scatter_matrix(data, alpha = 0.3, figsize = (8, 8), diagonal = 'kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the directions are even more obvious and the distributions are more even. To further inspect the correlation we plot a correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the correlations between the features\n",
    "ax = plt.axes()\n",
    "sns.heatmap(data.corr(), ax=ax)\n",
    "ax.set_title('Correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Did you find any correlations? Can you come up with some customers that would show such a pattern? Examples of establishments include places like markets, cafes, delis, wholesale retailers, among many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the main directions of the data (principal component analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it is useful to eliminate features that do not add much information to the model and we try to reduce the independent variables. We can do this with principal component analysis. Simplified, principal component analysis is finding the main (principal) directions (components) in the n-dimensional feature vector space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin: PCA Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create some random data with a clear direction\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=.6)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we calculate the first two principal components\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And display the first two principal components\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we can transform the data onto these new directions\n",
    "X_transformed = pca.transform(X)\n",
    "plt.scatter(X_transformed[:, 0], X_transformed[:, 1], alpha=0.2)\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second principal component')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End: PCA Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore the first 6 principal components (dimensions) and examine their variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA by fitting the good data with the same number of dimensions as features\n",
    "pca = PCA(n_components=6, random_state=42).fit(data)\n",
    "\n",
    "# Generate PCA results plot\n",
    "pca_results = vs.pca_results(data, pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: How many dimensions are explaning the most variance in the data? We can see that two dimensions are explaining more than 70% of the variance in our data set. In two dimension it is also possible to visualize our data set. Therefore we transform our data onto these two first principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply PCA by fitting the good data with only two dimensions\n",
    "# pca = PCA(...).fit(data)\n",
    "\n",
    "# Transform the good data using the PCA fit above\n",
    "reduced_data = pca.transform(data)\n",
    "\n",
    "# Transform samples as well\n",
    "reduced_samples = pca.transform(samples)\n",
    "\n",
    "# Create a DataFrame for the reduced data\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n",
    "vs.biplot(data, reduced_data, pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous plot show a biplot. A biplot is a scatterplot where each data point is represented by its scores along the principal components. The axes are the principal components (in this case Dimension 1 and Dimension 2). In addition, the biplot shows the projection of the original features along the components. A biplot can help us interpret the reduced dimensions of the data, and discover relationships between the principal components and original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Determine number of clusters in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: With our reduced data we are now able to apply clustering algorithms much more effective and explore the clusters on a 2d plane. Do you see a structure or how would you cluster the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the following cell you can experiment with various number of clusters by applying a gaussian mixture model (http://scikit-learn.org/stable/modules/mixture.html#gmm) and examining the silhouette score of the clusterings. The silhouette score (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) is a performance measure for clustering algorithms. The bigger the number, the smaller the intra cluster distance (1=best, -1=worst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Apply your clustering algorithm of choice to the reduced data \n",
    "# TODO: Experiment with different number of clusters\n",
    "clusterer = GaussianMixture(n_components=1).fit(reduced_data)\n",
    "\n",
    "# Predict the cluster for each data point\n",
    "preds = clusterer.predict(reduced_data)\n",
    "\n",
    "# Find the cluster centers\n",
    "centers = clusterer.means_\n",
    "\n",
    "# Calculate the mean silhouette coefficient for the number of clusters chosen\n",
    "score = silhouette_score(reduced_data, preds)\n",
    "print('Silhouette score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: If you have the number of clusters with the best shilouette score (and solid reasoning) visualize the clusters with the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display the results of the clustering from implementation\n",
    "vs.cluster_results(reduced_data, preds, centers, reduced_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Now you can compare your clusters with the underlying data structure by executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vs.channel_results(reduced_data, outliers, reduced_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation! You have now completed a complete clustering example with:\n",
    "* Data exploration\n",
    "* Data preprocessing\n",
    "* Principal component analysis\n",
    "* Dimensionality reduction\n",
    "* Applying a clustering algorithm (Gaussian Mixture Model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
