{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning boolean operations with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will learn the basics of neural networks by building very simple networks that can learn the logical operations. We start with a perceptron and add some hidden layers for solving the logical operations XOR and XNOR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a logical OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np \n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define logical or (only for training!)\n",
    "def logical_or(values):\n",
    "    if values[0] == 1 or values[1] == 1:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some training data\n",
    "X = [[random.randint(0,1), random.randint(0,1)] for x in range(1000)]\n",
    "y = [logical_or(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print our input and output data\n",
    "print('Input vectors (X): %s' % X[:5])\n",
    "print('Target variable (y): %s' % y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining our trivial network with a single layer, a tanh activation function and a bias unit\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(2,), activation='tanh', use_bias=True))\n",
    "model.compile(optimizer=\"SGD\", loss=\"mean_squared_error\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our model with the generated training data\n",
    "model.fit(np.array(X), np.array(y), epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our model with some validation data\n",
    "model.predict(np.array([[1,0],[1,1],[0,0],[0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the weights of the model\n",
    "for layer in model.layers:\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a logical AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We define logical and (only for training!)\n",
    "def logical_and(values):\n",
    "    if values[0] == 1 and values[1] == 1:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some training data\n",
    "X = [[random.randint(0,1), random.randint(0,1)] for x in range(1000)]\n",
    "y = [logical_and(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print our input and output data\n",
    "print('Input vectors (X): %s' % X[:5])\n",
    "print('Target variable (y): %s' % y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining our trivial network with a single layer, a tanh activation function and a bias unit\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(2,), activation='tanh', use_bias=True))\n",
    "model.compile(optimizer=\"SGD\", loss=\"mean_squared_error\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our model with the generated training data\n",
    "model.fit(np.array(X), np.array(y), epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate our model with some validation data\n",
    "model.predict(np.array([[1,0],[1,1],[0,0],[0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the weights of the model\n",
    "for layer in model.layers:\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training a logical XNOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We define logical xnor (only for training!)\n",
    "def logical_xnor(values):\n",
    "    if values[0] == 1 and values[1] == 1:\n",
    "        return 1\n",
    "    if values[0] == 0 and values[1] == 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some training data\n",
    "X = [[random.randint(0,1), random.randint(0,1)] for x in range(1000)]\n",
    "y = [logical_xnor(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thats what xnor is\n",
    "[logical_xnor(x) for x in [[1,0],[1,1],[0,0],[0,1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining our trivial network with a single layer, a tanh activation function and a bias unit\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_shape=(2,), activation='tanh', use_bias=True))\n",
    "model.add(Dense(1, activation='tanh', use_bias=True))\n",
    "model.compile(optimizer=\"SGD\", loss=\"mean_squared_error\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our model with the generated training data\n",
    "model.fit(np.array(X), np.array(y), epochs=100, verbose=1, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate our model with some validation data\n",
    "model.predict(np.array([[1,0],[1,1],[0,0],[0,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TODO: Training a logical XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know should be in the position to implement a neural network which learns the logical XOR function. The boilerplate code for data generation, the XOR function and the fitting and validation of the model has been provided for you. All you need to do is define the layers in the corresponding cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We define logical xor (only for training!)\n",
    "def logical_xor(values):\n",
    "    if values[0] == 0 and values[1] == 1:\n",
    "        return 1\n",
    "    if values[0] == 1 and values[1] == 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some training data\n",
    "X = [[random.randint(0,1), random.randint(0,1)] for x in range(1000)]\n",
    "y = [logical_xor(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print our input and output data\n",
    "print('Input vectors (X): %s' % X[:8])\n",
    "print('Target variable (y): %s' % y[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create testing and validation split\n",
    "X_train = X[:500]\n",
    "y_train = y[:500]\n",
    "X_validation = X[500:]\n",
    "y_validation = y[500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Define the architecture for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Create the model by defining the necessary layers\n",
    "\n",
    "# Compile the model initialize optimizers\n",
    "model.compile(optimizer=\"Adam\", loss=\"mean_squared_error\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint   \n",
    "import livelossplot as lp\n",
    "\n",
    "plot_learning = lp.PlotLossesKeras()\n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='xor.model.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "\n",
    "callbacks = [checkpointer, plot_learning]\n",
    "\n",
    "# Training our model with the generated training data\n",
    "model.fit(np.array(X), np.array(y), validation_data=(np.array(X_validation), np.array(y_validation)), \n",
    "          callbacks = callbacks, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights that yielded the best validation accuracy\n",
    "model.load_weights('xor.model.weights.best.hdf5')\n",
    "\n",
    "# Evaluate our model with some validation data\n",
    "model.predict(np.array([[1,0],[1,1],[0,0],[0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [[random.randint(0,1), random.randint(0,1)] for x in range(50)]\n",
    "y_test = [logical_xor(x) for x in X_test]\n",
    "score = model.evaluate(np.array(X_test), np.array(y_test), verbose=1)\n",
    "print('\\n', 'Model Accuracy: %.3f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation! You have mastered your first neural networks by training some logical operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
