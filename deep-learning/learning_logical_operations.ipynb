{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning boolean operations with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np \n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define logical or (only for training!)\n",
    "def logical_or(values):\n",
    "    if values[0] == 1 or values[1] == 1:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some training data\n",
    "X = [[random.randint(0,1), random.randint(0,1)] for x in range(1000)]\n",
    "y = [logical_or(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vectors (X): [[0, 0], [0, 1], [1, 1], [1, 0], [1, 1]]\n",
      "Target variable (y): [0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Print our input and output data\n",
    "print('Input vectors (X): %s' % X[:5])\n",
    "print('Target variable (y): %s' % y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining our trivial network with a single layer, a tanh activation function and a bias unit\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(2,), activation='tanh', use_bias=True))\n",
    "model.compile(optimizer=\"SGD\", loss=\"mean_squared_error\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 189us/step - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0093 - acc: 1.0000\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0084 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0081 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.0078 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 0.0077 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f03c529d940>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training our model with the generated training data\n",
    "model.fit(np.array(X), np.array(y), epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.88151067],\n",
       "       [ 0.99109834],\n",
       "       [ 0.04673347],\n",
       "       [ 0.87854779]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate our model with some validation data\n",
    "model.predict(np.array([[1,0],[1,1],[0,0],[0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.33573616],\n",
      "       [ 1.32259905]], dtype=float32), array([ 0.04676754], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Visualize the weights of the model\n",
    "for layer in model.layers:\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We define logical and (only for training!)\n",
    "def logical_and(values):\n",
    "    if values[0] == 1 and values[1] == 1:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some training data\n",
    "X = [[random.randint(0,1), random.randint(0,1)] for x in range(1000)]\n",
    "y = [logical_and(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vectors (X): [[0, 0], [0, 1], [1, 0], [1, 0], [1, 1]]\n",
      "Target variable (y): [0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Print our input and output data\n",
    "print('Input vectors (X): %s' % X[:5])\n",
    "print('Target variable (y): %s' % y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining our trivial network with a single layer, a tanh activation function and a bias unit\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(2,), activation='tanh', use_bias=True))\n",
    "model.compile(optimizer=\"SGD\", loss=\"mean_squared_error\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 0s 239us/step - loss: 0.2229 - acc: 0.5060\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.1741 - acc: 0.7340\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.1377 - acc: 0.7850\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.1147 - acc: 0.7850\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.1016 - acc: 0.8660: 0s - loss: 0.1039 - acc: 0.838\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0951 - acc: 1.0000\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0913 - acc: 1.0000\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0888 - acc: 1.0000\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0870 - acc: 1.0000\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0855 - acc: 1.0000\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0844 - acc: 1.0000\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0835 - acc: 1.0000\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0826 - acc: 1.0000\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.0819 - acc: 1.0000\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0814 - acc: 1.0000\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.0809 - acc: 1.0000\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.0805 - acc: 1.0000\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s 83us/step - loss: 0.0801 - acc: 1.0000\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s 89us/step - loss: 0.0799 - acc: 1.0000\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.0796 - acc: 1.0000\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s 82us/step - loss: 0.0794 - acc: 1.0000\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.0792 - acc: 1.0000\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s 87us/step - loss: 0.0791 - acc: 1.0000\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s 82us/step - loss: 0.0790 - acc: 1.0000\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s 79us/step - loss: 0.0789 - acc: 1.0000\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 0.0788 - acc: 1.0000\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0787 - acc: 1.0000\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s 82us/step - loss: 0.0786 - acc: 1.0000\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s 76us/step - loss: 0.0786 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.0785 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 0.0785 - acc: 1.0000\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.0785 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s 79us/step - loss: 0.0785 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.0784 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 0.0784 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0784 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0783 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0783 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0784 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0783 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 0.0783 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s 80us/step - loss: 0.0783 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.0783 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0783 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s 77us/step - loss: 0.0783 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0783 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s 76us/step - loss: 0.0783 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 0.0783 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s 80us/step - loss: 0.0783 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0783 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f03c0221e10>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training our model with the generated training data\n",
    "model.fit(np.array(X), np.array(y), epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.28036863],\n",
       "       [ 0.63472402],\n",
       "       [-0.24683933],\n",
       "       [ 0.20616394]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate our model with some validation data\n",
    "model.predict(np.array([[1,0],[1,1],[0,0],[0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.54012638],\n",
      "       [ 0.46120593]], dtype=float32), array([-0.25204426], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Visualize the weights of the model\n",
    "for layer in model.layers:\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Implement XNOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We define logical xnor (only for training!)\n",
    "def logical_xnor(values):\n",
    "    if values[0] == 1 and values[1] == 1:\n",
    "        return 1\n",
    "    if values[0] == 0 and values[1] == 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some training data\n",
    "X = [[random.randint(0,1), random.randint(0,1)] for x in range(1000)]\n",
    "y = [logical_xnor(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thats what xnor is\n",
    "[logical_xnor(x) for x in [[1,0],[1,1],[0,0],[0,1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining our trivial network with a single layer, a tanh activation function and a bias unit\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_shape=(2,), activation='tanh', use_bias=True))\n",
    "model.add(Dense(1, activation='tanh', use_bias=True))\n",
    "model.compile(optimizer=\"SGD\", loss=\"mean_squared_error\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.3342 - acc: 0.4900\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 0s 254us/step - loss: 0.2509 - acc: 0.4970\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 0s 240us/step - loss: 0.2488 - acc: 0.5730\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 0s 239us/step - loss: 0.2486 - acc: 0.5150\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 0s 253us/step - loss: 0.2480 - acc: 0.6270\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 0s 257us/step - loss: 0.2473 - acc: 0.5680\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 0s 235us/step - loss: 0.2470 - acc: 0.6170\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 0s 240us/step - loss: 0.2463 - acc: 0.6870\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 0s 249us/step - loss: 0.2456 - acc: 0.7290\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 0s 252us/step - loss: 0.2449 - acc: 0.7080\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 0s 255us/step - loss: 0.2441 - acc: 0.7240\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 0s 244us/step - loss: 0.2426 - acc: 0.6660\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 0s 237us/step - loss: 0.2418 - acc: 0.7480\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 0s 249us/step - loss: 0.2403 - acc: 0.7190\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 0s 241us/step - loss: 0.2387 - acc: 0.7480\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 0s 253us/step - loss: 0.2370 - acc: 0.7480\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 0s 242us/step - loss: 0.2350 - acc: 0.7480\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 0s 222us/step - loss: 0.2329 - acc: 0.7480\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 0s 236us/step - loss: 0.2304 - acc: 0.7480\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 0s 240us/step - loss: 0.2278 - acc: 0.7480\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 0s 233us/step - loss: 0.2250 - acc: 0.7480\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 0s 225us/step - loss: 0.2222 - acc: 0.7480\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 0s 229us/step - loss: 0.2193 - acc: 0.7480\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 0s 235us/step - loss: 0.2162 - acc: 0.7480\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 0s 244us/step - loss: 0.2132 - acc: 0.7480\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 0s 236us/step - loss: 0.2102 - acc: 0.7480\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 0s 240us/step - loss: 0.2070 - acc: 0.7480\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 0s 262us/step - loss: 0.2040 - acc: 0.7480\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 0s 241us/step - loss: 0.2006 - acc: 0.7480\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 0s 245us/step - loss: 0.1979 - acc: 0.7480\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 0s 232us/step - loss: 0.1949 - acc: 0.7480\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 0s 244us/step - loss: 0.1918 - acc: 0.7480\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 0s 254us/step - loss: 0.1890 - acc: 0.7480\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 0s 252us/step - loss: 0.1861 - acc: 0.7480\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 0s 238us/step - loss: 0.1831 - acc: 0.7480\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 0s 244us/step - loss: 0.1800 - acc: 0.7480\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 0s 235us/step - loss: 0.1767 - acc: 0.7480\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 0s 236us/step - loss: 0.1731 - acc: 0.7480\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 0s 222us/step - loss: 0.1693 - acc: 0.7480\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 0s 227us/step - loss: 0.1650 - acc: 0.7480\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 0s 227us/step - loss: 0.1597 - acc: 0.7480\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 0s 225us/step - loss: 0.1533 - acc: 0.7480\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 0s 229us/step - loss: 0.1453 - acc: 0.7480\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 0s 231us/step - loss: 0.1358 - acc: 0.8840\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 0s 234us/step - loss: 0.1247 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 0s 232us/step - loss: 0.1123 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 0s 236us/step - loss: 0.0996 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 0s 224us/step - loss: 0.0871 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 0s 232us/step - loss: 0.0753 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 0s 227us/step - loss: 0.0647 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 0s 223us/step - loss: 0.0553 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 0s 223us/step - loss: 0.0472 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 0s 220us/step - loss: 0.0402 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 0s 225us/step - loss: 0.0344 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 0s 223us/step - loss: 0.0296 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 0s 217us/step - loss: 0.0256 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 0s 237us/step - loss: 0.0223 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 0s 243us/step - loss: 0.0195 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 0s 242us/step - loss: 0.0172 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 0s 239us/step - loss: 0.0153 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 0s 236us/step - loss: 0.0137 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 0s 242us/step - loss: 0.0123 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 0s 242us/step - loss: 0.0111 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 0s 239us/step - loss: 0.0101 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 0s 241us/step - loss: 0.0093 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 0s 247us/step - loss: 0.0085 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 0s 260us/step - loss: 0.0078 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 0s 245us/step - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 0s 234us/step - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 0s 251us/step - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 0s 242us/step - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 0s 252us/step - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 0s 237us/step - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 0s 244us/step - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 0s 240us/step - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 0s 243us/step - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 0s 253us/step - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 0s 253us/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 0s 272us/step - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 0s 239us/step - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 0s 231us/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 0s 230us/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 0s 223us/step - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 0s 243us/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 0s 245us/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 0s 230us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 0s 236us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 0s 257us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 0s 245us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 0s 239us/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 0s 241us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 0s 238us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 0s 243us/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 0s 240us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 0s 269us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 0s 241us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 0s 243us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 0s 238us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 0s 246us/step - loss: 0.0019 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0356c8c208>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training our model with the generated training data\n",
    "model.fit(np.array(X), np.array(y), epochs=100, verbose=1, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0085232 ],\n",
       "       [ 0.94722325],\n",
       "       [ 0.9309516 ],\n",
       "       [ 0.00631127]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate our model with some validation data\n",
    "model.predict(np.array([[1,0],[1,1],[0,0],[0,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Exercise: Implement XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We define logical xor (only for training!)\n",
    "def logical_xor(values):\n",
    "    if values[0] == 0 and values[1] == 1:\n",
    "        return 1\n",
    "    if values[0] == 1 and values[1] == 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some training data\n",
    "X = [[random.randint(0,1), random.randint(0,1)] for x in range(1000)]\n",
    "y = [logical_xor(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vectors (X): [[1, 1], [1, 1], [1, 0], [1, 1], [0, 0], [1, 0], [1, 1], [0, 1]]\n",
      "Target variable (y): [0, 0, 1, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Print our input and output data\n",
    "print('Input vectors (X): %s' % X[:8])\n",
    "print('Target variable (y): %s' % y[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create testing and validation split\n",
    "X_train = X[:500]\n",
    "y_train = y[:500]\n",
    "X_validation = X[500:]\n",
    "y_validation = y[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# TODO: Implement the neural network and achieve an accuracy greater than 0.7\n",
    "# -------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint   \n",
    "import livelossplot as lp\n",
    "\n",
    "plot_learning = lp.PlotLossesKeras()\n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='xor.model.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "\n",
    "callbacks = [checkpointer, plot_learning]\n",
    "\n",
    "# Training our model with the generated training data\n",
    "model.fit(np.array(X), np.array(y), validation_data=(np.array(X_validation), np.array(y_validation)), \n",
    "          callbacks = callbacks, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the weights that yielded the best validation accuracy\n",
    "model.load_weights('xor.model.weights.best.hdf5')\n",
    "\n",
    "# Evaluate our model with some validation data\n",
    "model.predict(np.array([[1,0],[1,1],[0,0],[0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = [[random.randint(0,1), random.randint(0,1)] for x in range(50)]\n",
    "y_test = [logical_xor(x) for x in X_test]\n",
    "score = model.evaluate(np.array(X_test), np.array(y_test), verbose=1)\n",
    "print('\\n', 'Model Accuracy: %.3f' % score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
